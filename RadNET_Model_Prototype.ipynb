{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RadNET_Model_Prototype.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNS8/EBGZJd8TER7jMNOavC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astroChance/RadNET/blob/black_white/RadNET_Model_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d17cxD3yulS"
      },
      "source": [
        "# Altering for single-channel B&W images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B2hHrFtW1TT"
      },
      "source": [
        "\"\"\"\n",
        "Model based on Pix2Pix structure\n",
        "\n",
        "utilizes Unet generator with skip connections\n",
        "\n",
        "pix2pix tutorial (Unet generator example)  https://www.tensorflow.org/tutorials/generative/pix2pix\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubOBL062XbCY"
      },
      "source": [
        "# Library Imports and Data Access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcmUQeeUXf3u"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "from IPython import display\n",
        "from PIL import Image\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9aV5_W1XkJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3623b8a-427e-4c93-bf98-57f134f34dbf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrcqNEEBYrt6"
      },
      "source": [
        "## GPU / TPU checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W41GZ4JmGMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "980d03b5-2083-4dad-ab68-063fe19cc710"
      },
      "source": [
        "## Test that GPU is enabled\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7K3j9guJ9Je"
      },
      "source": [
        "## Initialize TPU\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4V04OTHn7Pv"
      },
      "source": [
        "## Check available memory\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1sNRNXoi6Q"
      },
      "source": [
        "## Check GPU memory\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWqp7O-VYxX1"
      },
      "source": [
        "# Download data to local VM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-jTEp3Ot7iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a677f12-52d0-4cca-c1ba-abf49579ffc8"
      },
      "source": [
        "## Use gdown to copy zip files to local VM\n",
        "## Unpack zip files on local VM\n",
        "\n",
        "## Only use set 1 or set 2, each is 20k images\n",
        "# Set 1\n",
        "!gdown --id 1V_RDJIf8hQbmbznrkVwv197MDfHGB8Gu\n",
        "!gdown --id 1eO_rHzznTZukW0J6wFdRgIuYyhqP6eis\n",
        "\n",
        "_2D_folder_local = '/content/2D_Production_Min12_Max1_Training_BWSet1.zip'\n",
        "\n",
        "_3D_folder_local = '/content/3D_Production_Min12_Max1_Training_BWSet1.zip'\n",
        "\n",
        "target_dir = '/content'\n",
        "shutil.unpack_archive(_2D_folder_local, target_dir)\n",
        "shutil.unpack_archive(_3D_folder_local, target_dir)\n",
        "\n",
        "# Set 2\n",
        "!gdown --id 1ToulzcGXOwyb3vkRCx1X0Vj7Dx04JTwL\n",
        "!gdown --id 1N-7aImVlvvi-gLo42ryYnAlTUNKeIHST\n",
        "\n",
        "_2D_folder_local = '/content/2D_Production_Min12_Max1_Training_BWSet2.zip'\n",
        "\n",
        "_3D_folder_local = '/content/3D_Production_Min12_Max1_Training_BWSet2.zip'\n",
        "\n",
        "target_dir = '/content'\n",
        "shutil.unpack_archive(_2D_folder_local, target_dir)\n",
        "shutil.unpack_archive(_3D_folder_local, target_dir)\n",
        "\n",
        "# Print sizes of training directories\n",
        "total = 0\n",
        "for path, dirs, files in os.walk(_2D_folder_local.split('.')[0]):\n",
        "    for f in files:\n",
        "        fp = os.path.join(path, f)\n",
        "        total += os.path.getsize(fp)\n",
        "print(\"Size of 2D directory: \", (total/1000000), \"MB\")\n",
        "\n",
        "total = 0\n",
        "for path, dirs, files in os.walk(_3D_folder_local.split('.')[0]):\n",
        "    for f in files:\n",
        "        fp = os.path.join(path, f)\n",
        "        total += os.path.getsize(fp)\n",
        "print(\"Size of 3D directory: \", (total/1000000), \"MB\")\n",
        "\n",
        "\n",
        "## Links to zip files on Drive\n",
        "# 2D Set 1 zip\n",
        "# https://drive.google.com/file/d/1V_RDJIf8hQbmbznrkVwv197MDfHGB8Gu/view?usp=sharing\n",
        "# 2D Set 2 zip\n",
        "# https://drive.google.com/file/d/1ToulzcGXOwyb3vkRCx1X0Vj7Dx04JTwL/view?usp=sharing\n",
        "\n",
        "# 3D Set 1 zip\n",
        "# https://drive.google.com/file/d/1eO_rHzznTZukW0J6wFdRgIuYyhqP6eis/view?usp=sharing\n",
        "# 3D Set 2 zip\n",
        "# https://drive.google.com/file/d/1N-7aImVlvvi-gLo42ryYnAlTUNKeIHST/view?usp=sharing"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1V_RDJIf8hQbmbznrkVwv197MDfHGB8Gu\n",
            "To: /content/2D_Production_Min12_Max1_Training_BWSet1.zip\n",
            "108MB [00:01, 92.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eO_rHzznTZukW0J6wFdRgIuYyhqP6eis\n",
            "To: /content/3D_Production_Min12_Max1_Training_BWSet1.zip\n",
            "108MB [00:01, 101MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ToulzcGXOwyb3vkRCx1X0Vj7Dx04JTwL\n",
            "To: /content/2D_Production_Min12_Max1_Training_BWSet2.zip\n",
            "108MB [00:01, 81.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N-7aImVlvvi-gLo42ryYnAlTUNKeIHST\n",
            "To: /content/3D_Production_Min12_Max1_Training_BWSet2.zip\n",
            "108MB [00:01, 88.6MB/s]\n",
            "Size of 2D directory:  118.999641 MB\n",
            "Size of 3D directory:  118.25319 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgW_uO7DX8hW"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-xvMqioYAL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6c6f59-22b1-4beb-af0b-a0f0f17e469b"
      },
      "source": [
        "## Define local VM paths to image files\n",
        "\n",
        "set_1_or_2 = 1  # define if using set 1 or set 2, 20k images each\n",
        "\n",
        "if set_1_or_2 == 1:\n",
        "  _2D_imgs_path = '/content/2D_Production_Min12_Max1_Training_BWSet1'\n",
        "  _3D_imgs_path = '/content/3D_Production_Min12_Max1_Training_BWSet1'\n",
        "elif set_1_or_2 == 2:\n",
        "  _2D_imgs_path = '/content/2D_Production_Min12_Max1_Training_BWSet2'\n",
        "  _3D_imgs_path = '/content/3D_Production_Min12_Max1_Training_BWSet2'\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "## Iterate through folders, create list of filepaths for images\n",
        "TwoDImages = []\n",
        "for root, dirs, files in os.walk(_2D_imgs_path):\n",
        "      for name in files:\n",
        "          if name.endswith(\".png\"):\n",
        "              filename = os.path.join(root, name)\n",
        "              TwoDImages.append(filename)\n",
        "\n",
        "ThreeDImages = []\n",
        "for root, dirs, files in os.walk(_3D_imgs_path):\n",
        "      for name in files:\n",
        "          if name.endswith(\".png\"):\n",
        "              filename = os.path.join(root, name)\n",
        "              ThreeDImages.append(filename)\n",
        "\n",
        "TwoDImages = sorted(TwoDImages, key=lambda x: int(x.split('/')[-1].split('_')[0]))\n",
        "ThreeDImages = sorted(ThreeDImages, key=lambda x: int(x.split('/')[-1].split('_')[0]))\n",
        "\n",
        "# Deleted subsetting of images, trying to fit all 20k\n",
        "# black and white images into memory\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time to completion (s): \", end-start)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to completion (s):  0.15360212326049805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZaTe7avpikL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ec43e7-7b92-4016-f2d1-ae1ea40d3ffc"
      },
      "source": [
        "## Check size of file lists, check that trace pair number (leading digits)\n",
        "## is same between 2D and 3D file\n",
        "\n",
        "print(len(TwoDImages))\n",
        "print(len(ThreeDImages))\n",
        "print(TwoDImages[-1])\n",
        "print(ThreeDImages[-1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000\n",
            "20000\n",
            "/content/2D_Production_Min12_Max1_Training_BWSet1/19999_2090401000_2D_238.png\n",
            "/content/3D_Production_Min12_Max1_Training_BWSet1/19999_2090401000_3D_217121.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFiusYz81sBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073964f3-e01f-40fa-a00b-e3543c6e0b46"
      },
      "source": [
        "######\n",
        "# Create TensorFLow Dataset from images\n",
        "######\n",
        "\n",
        "## Parameters used throughout Preparation and Model Build\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE0 = 256    # Initial model implementation only works with 256x256 - revisit up/down layers for other sizes\n",
        "IMG_SIZE1 = 256\n",
        "OUTPUT_CHANNELS = 1  # changing to one channel for B&W images\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "tr_data3d = np.zeros((len(ThreeDImages), IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS), dtype=np.float32)\n",
        "counter = 0\n",
        "for image in ThreeDImages:\n",
        "  with Image.open(image) as img:\n",
        "    img = np.array(img)\n",
        "    img=img[:,:,:1]  # only take first channel of B&W image\n",
        "    img=(img / 127.5) - 1\n",
        "    tr_data3d[counter] = img\n",
        "    del img\n",
        "    counter += 1\n",
        "\n",
        "tr_data2d = np.zeros((len(TwoDImages), IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS), dtype=np.float32)\n",
        "counter = 0\n",
        "for image in TwoDImages:\n",
        "  with Image.open(image) as img:\n",
        "    img = np.array(img)\n",
        "    img=img[:,:,:1]   # only take first channel of B&W image\n",
        "    img=(img / 127.5) - 1\n",
        "    tr_data2d[counter] = img\n",
        "    del img\n",
        "    counter += 1\n",
        "\n",
        "data_tf_2D = tf.convert_to_tensor(tr_data2d, np.float32)\n",
        "data_tf_3D = tf.convert_to_tensor(tr_data3d, np.float32)\n",
        "del tr_data2d\n",
        "del tr_data3d\n",
        "\n",
        "train_dataset2d_0 = tf.data.Dataset.from_tensor_slices(data_tf_2D)\n",
        "train_dataset2d = train_dataset2d_0.batch(BATCH_SIZE)\n",
        "\n",
        "train_dataset3d_0 = tf.data.Dataset.from_tensor_slices(data_tf_3D)\n",
        "train_dataset3d = train_dataset3d_0.batch(BATCH_SIZE)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time to completion (s): \", end-start)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to completion (s):  63.73155665397644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NkQ5LDpZZ8R"
      },
      "source": [
        "# Define Deep Learning Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUt251j_Z8ho"
      },
      "source": [
        "# Define discriminator and generator models\n",
        "# Define loss and optimizers\n",
        "# Instantiate models\n",
        "\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "\n",
        "\n",
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
        "\n",
        "\n",
        "# define loss and optimizers\n",
        "\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss\n",
        "\n",
        "\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "\n",
        "# Instantiate the models  (toggle on for GPU)\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjaEgP3bK6a5"
      },
      "source": [
        "#################\n",
        "#### ENABLE TPU VERSION OF MODEL\n",
        "\n",
        "#### ONLY USE WITH TPU\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  generator = Generator()\n",
        "  discriminator = Discriminator()\n",
        "\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "# generator = tf.contrib.tpu.keras_to_tpu_model(\n",
        "#     generator,\n",
        "#     strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "# discriminator = tf.contrib.tpu.keras_to_tpu_model(\n",
        "#     discriminator,\n",
        "#     strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNRPfnE5aS-P"
      },
      "source": [
        "## Display graphical model description\n",
        "\n",
        "# Generator().summary()\n",
        "# Discriminator().summary()\n",
        "\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
        "# tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y-dDepoak5S"
      },
      "source": [
        "# Define Training Steps and Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-6MC4HCan4C"
      },
      "source": [
        "# set up checkpoints and define training / fit steps\n",
        "\n",
        "log_dir=\"/content/drive/My Drive/RadNET/logs/Production_Run2\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "checkpoint_dir = '/content/drive/My Drive/RadNET/training_checkpoints/Production_Run2'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "def fit(train_3d, train_2d, epochs, chkpt_save):\n",
        "    for epoch in range(epochs):\n",
        "        display.clear_output(wait=True)\n",
        "        try:\n",
        "            print ('Time taken for epoch {} was {} sec\\n'.format(epoch - 1,\n",
        "                                                        time.time()-start))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        start = time.time()\n",
        "        print(\"Epoch \", epoch, \" running...\")\n",
        "\n",
        "        n=0\n",
        "        # Train (separate Datasets for input and target)\n",
        "        for input_image, target in tf.data.Dataset.zip((train_3d, train_2d)):\n",
        "            train_step(input_image, target, epoch)\n",
        "            if (n+1) % 100 == 0:\n",
        "                print('.', end='')\n",
        "            n +=1 \n",
        "    \n",
        "        print()\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    # saving (checkpoint) the model every chkpt_save epochs\n",
        "        if (epoch + 1) % chkpt_save == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9NjoAjpbSp7"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GRizQdMbUN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b22d182-37d5-4b6b-82eb-650852366378"
      },
      "source": [
        "# Load latest checkpoint if picking up / continuing training \n",
        "# Skip if starting from scratch\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0a1280ddd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gw_tqi4pMI9",
        "outputId": "4d5e354e-99b4-4cae-c817-e79248a8b233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# Launch Tensorboard to monitor progress\r\n",
        "\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 2).\n",
              "Contents of stderr:\n",
              "2020-12-23 18:14:24.125937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
              "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
              "                   [--host ADDR] [--bind_all] [--port PORT]\n",
              "                   [--purge_orphaned_data BOOL] [--db URI] [--db_import]\n",
              "                   [--inspect] [--version_tb] [--tag TAG] [--event_file PATH]\n",
              "                   [--path_prefix PATH] [--window_title TEXT]\n",
              "                   [--max_reload_threads COUNT] [--reload_interval SECONDS]\n",
              "                   [--reload_task TYPE] [--reload_multifile BOOL]\n",
              "                   [--reload_multifile_inactive_secs SECONDS]\n",
              "                   [--generic_data TYPE]\n",
              "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
              "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
              "                   {serve,dev} ...\n",
              "tensorboard: error: invalid choice: 'Drive/RadNET/logs/Production_Run2' (choose from 'serve', 'dev')"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSlUzWVjbXXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0722cea-f280-44c0-ed02-ee80d38cf3c6"
      },
      "source": [
        "# Define how many epochs to run model\n",
        "EPOCHS = 39\n",
        "chkpt_save = 20\n",
        "\n",
        "# Run the model\n",
        "fit(train_dataset3d, train_dataset2d, EPOCHS, chkpt_save)\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for epoch 37 was 149.5414342880249 sec\n",
            "\n",
            "Epoch  38  running...\n",
            "...\n",
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CdiaENRc35V"
      },
      "source": [
        "# Run Predictions on Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykTl0sfEm775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b1615c-103a-4907-8666-518427e2e97a"
      },
      "source": [
        "### Use gdown to copy zip files to local VM\n",
        "## Unpack zip files on local VM\n",
        "  \n",
        "!gdown --id 1cYpHpNFLKN_Ojei9vnMGeJOCNnLbYYkx\n",
        "\n",
        "_3D_Test_folder_local = '/content/3D_Production_Min12_Max1_Testing.zip'\n",
        "\n",
        "target_dir = '/content'\n",
        "shutil.unpack_archive(_3D_Test_folder_local, target_dir)\n",
        "\n",
        "## Link to zip file on Drive\n",
        "# 3D\n",
        "# https://drive.google.com/file/d/1cYpHpNFLKN_Ojei9vnMGeJOCNnLbYYkx/view?usp=sharing"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JE76ICdBm4jXUBTO6Gnlp73m17cWWtuZ\n",
            "To: /content/2D_Production_Min12_Max1_Testing.zip\n",
            "2.68MB [00:00, 136MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cYpHpNFLKN_Ojei9vnMGeJOCNnLbYYkx\n",
            "To: /content/3D_Production_Min12_Max1_Testing.zip\n",
            "2.69MB [00:00, 85.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKKC-53KdpAq"
      },
      "source": [
        "# Run GAN Frequency Enhancement on 3D images\n",
        "\n",
        "## local VM data:\n",
        "_3D_test_path = '/content/3D_Production_Min12_Max1_Testing'\n",
        "\n",
        "output_prediction_path = \"/content/drive/My Drive/RadNET/GAN data/Production_Run2_BW/Predictions_v122320_lowEpochs/\"\n",
        "\n",
        "\n",
        "\n",
        "TestImages = []\n",
        "for root, dirs, files in os.walk(_3D_test_path):\n",
        "    for name in files:\n",
        "        if name.endswith(\".png\"):\n",
        "            filename = os.path.join(root, name)\n",
        "            name = name.split('.')[0]\n",
        "            TestImages.append([name, filename])\n",
        "\n",
        "## Make sure these parameters match image generation step\n",
        "colormap = 'binary'\n",
        "min_val = -12\n",
        "max_val = 1\n",
        "shading = 'flat'\n",
        "\n",
        "for image in TestImages:\n",
        "    with Image.open(image[1]) as img:\n",
        "      img = np.array(img)\n",
        "      img=img[:,:,:1]\n",
        "      img=(img / 127.5) - 1\n",
        "      testing = np.expand_dims(img, axis=0)\n",
        "      model_test = generator.predict(testing)\n",
        "      model_test = np.squeeze(model_test)\n",
        "      model_test = (model_test + 1) * 127.5\n",
        "      model_test = model_test.astype(np.uint8)\n",
        "      plt.imsave(os.path.join(output_prediction_path,str(image[0]) + \"_prediction.png\"), \n",
        "                model_test, cmap=colormap, vmin=min_val, vmax=max_val)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}