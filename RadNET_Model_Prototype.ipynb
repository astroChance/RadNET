{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RadNET_Model_Prototype.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMM/8CeYaZd5FJbXO59VYjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astroChance/RadNET/blob/master/RadNET_Model_Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B2hHrFtW1TT"
      },
      "source": [
        "\"\"\"\n",
        "Model based on Pix2Pix structure\n",
        "\n",
        "utilizes Unet generator with skip connections\n",
        "\n",
        "pix2pix tutorial (Unet generator example)  https://www.tensorflow.org/tutorials/generative/pix2pix\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubOBL062XbCY"
      },
      "source": [
        "# Library Imports and Data Access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcmUQeeUXf3u"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "from IPython import display\n",
        "from PIL import Image\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9aV5_W1XkJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947117c5-ff6a-4357-c2ba-98efda3be4cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrcqNEEBYrt6"
      },
      "source": [
        "## GPU / TPU checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W41GZ4JmGMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "980d03b5-2083-4dad-ab68-063fe19cc710"
      },
      "source": [
        "## Test that GPU is enabled\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7K3j9guJ9Je"
      },
      "source": [
        "## Initialize TPU\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4V04OTHn7Pv"
      },
      "source": [
        "## Check available memory\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW1sNRNXoi6Q"
      },
      "source": [
        "## Check GPU memory\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWqp7O-VYxX1"
      },
      "source": [
        "# Download data to local VM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-jTEp3Ot7iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22d43a6-8283-4154-f77a-1bebc0364b5a"
      },
      "source": [
        "### Use gdown to copy zip files to local VM\n",
        "## Unpack zip files on local VM\n",
        "\n",
        "!gdown --id 17fOY0p2RayzycBCcdBAwD1n0dsZ2gaRr\n",
        "!gdown --id 17-He-AtD-BtPslRAspWe0Dh8eH-Taa5i\n",
        "\n",
        "_2D_folder_local = '/content/2D_Production_Min10_Max1_Training-20201023T183007Z-001.zip'\n",
        "_2D_target_dir = '/content/2D_training'\n",
        "\n",
        "_3D_folder_local = '/content/3D_Production_Min10_Max1_Training-20201023T182958Z-001.zip'\n",
        "_3D_target_dir = '/content/3D_training'\n",
        "\n",
        "shutil.unpack_archive(_2D_folder_local, _2D_target_dir)\n",
        "shutil.unpack_archive(_3D_folder_local, _3D_target_dir)\n",
        "\n",
        "# Print sizes of training directories\n",
        "total = 0\n",
        "for path, dirs, files in os.walk(_2D_target_dir):\n",
        "    for f in files:\n",
        "        fp = os.path.join(path, f)\n",
        "        total += os.path.getsize(fp)\n",
        "print(\"Size of 2D directory: \", (total/1000000), \"MB\")\n",
        "\n",
        "total = 0\n",
        "for path, dirs, files in os.walk(_3D_target_dir):\n",
        "    for f in files:\n",
        "        fp = os.path.join(path, f)\n",
        "        total += os.path.getsize(fp)\n",
        "print(\"Size of 3D directory: \", (total/1000000), \"MB\")\n",
        "\n",
        "\n",
        "## Links to zip files on Drive\n",
        "# 2D zip\n",
        "# https://drive.google.com/file/d/17fOY0p2RayzycBCcdBAwD1n0dsZ2gaRr/view?usp=sharing\n",
        "\n",
        "# 3D zip \n",
        "# https://drive.google.com/file/d/17-He-AtD-BtPslRAspWe0Dh8eH-Taa5i/view?usp=sharing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17fOY0p2RayzycBCcdBAwD1n0dsZ2gaRr\n",
            "To: /content/2D_Production_Min10_Max1_Training-20201023T183007Z-001.zip\n",
            "166MB [00:01, 122MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17-He-AtD-BtPslRAspWe0Dh8eH-Taa5i\n",
            "To: /content/3D_Production_Min10_Max1_Training-20201023T182958Z-001.zip\n",
            "155MB [00:04, 35.5MB/s]\n",
            "Size of 2D directory:  159.786722 MB\n",
            "Size of 3D directory:  148.51073 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgW_uO7DX8hW"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-xvMqioYAL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac1c0c6-e051-4202-8cb5-687e46b964bd"
      },
      "source": [
        "# ## Define drive locations (if streaming from mounted Drive)\n",
        "# _2D_imgs_path = \"/content/drive/My Drive/RadNET/GAN data/Production_Run1/2D_Production_Min10_Max1_Training\"\n",
        "# _3D_imgs_path = \"/content/drive/My Drive/RadNET/GAN data/Production_Run1/3D_Production_Min10_Max1_Training\"\n",
        "\n",
        "## Define local VM paths to image files\n",
        "_2D_imgs_path = '/content/2D_training/2D_Production_Min10_Max1_Training'\n",
        "_3D_imgs_path = '/content/3D_training/3D_Production_Min10_Max1_Training'\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "## Iterate through folders, create list of filepaths for images\n",
        "TwoDImages = []\n",
        "for root, dirs, files in os.walk(_2D_imgs_path):\n",
        "      for name in files:\n",
        "          if name.endswith(\".png\"):\n",
        "              filename = os.path.join(root, name)\n",
        "              TwoDImages.append(filename)\n",
        "\n",
        "ThreeDImages = []\n",
        "for root, dirs, files in os.walk(_3D_imgs_path):\n",
        "      for name in files:\n",
        "          if name.endswith(\".png\"):\n",
        "              filename = os.path.join(root, name)\n",
        "              ThreeDImages.append(filename)\n",
        "\n",
        "TwoDImages = sorted(TwoDImages, key=lambda x: int(x.split('/')[-1].split('_')[0]))\n",
        "ThreeDImages = sorted(ThreeDImages, key=lambda x: int(x.split('/')[-1].split('_')[0]))\n",
        "\n",
        "##### Defining training set size to fit into GPU memory\n",
        "## use multiple of batch size\n",
        "\n",
        "## if using static slice:\n",
        "# TwoDImages = TwoDImages[12800:]\n",
        "# ThreeDImages = ThreeDImages[12800:]\n",
        "\n",
        "## for random dataset:\n",
        "\n",
        "num_images = 6400\n",
        "\n",
        "rand_idxs = []\n",
        "\n",
        "while len(rand_idxs) < num_images:\n",
        "  idx = np.random.randint(0, len(TwoDImages)-1)\n",
        "  if idx in rand_idxs:\n",
        "    continue\n",
        "  rand_idxs.append(idx)\n",
        "\n",
        "TwoDImages = [TwoDImages[i] for i in rand_idxs]\n",
        "ThreeDImages = [ThreeDImages[i] for i in rand_idxs]\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time to completion (s): \", end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to completion (s):  0.4592747688293457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZaTe7avpikL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec93024-a715-46f3-e325-ec19aeb062ac"
      },
      "source": [
        "## Check size of file lists, check that trace pair number (leading digits)\n",
        "## is same between 2D and 3D file\n",
        "\n",
        "print(len(TwoDImages))\n",
        "print(len(ThreeDImages))\n",
        "print(TwoDImages[-1])\n",
        "print(ThreeDImages[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6400\n",
            "6400\n",
            "/content/2D_training/2D_Production_Min10_Max1_Training/17133_1308201000_2D_132.png\n",
            "/content/3D_training/3D_Production_Min10_Max1_Training/17133_1308201000_3D_215678.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFiusYz81sBa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c4875c-5d10-4130-9876-eb09e424a962"
      },
      "source": [
        "######\n",
        "# Create TensorFLow Dataset from images\n",
        "######\n",
        "\n",
        "## Parameters used throughout Preparation and Model Build\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE0 = 256    # Initial model implementation only works with 256x256 - revisit up/down layers for other sizes\n",
        "IMG_SIZE1 = 256\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "tr_data3d = np.zeros((len(ThreeDImages), IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS), dtype=np.float32)\n",
        "counter = 0\n",
        "for image in ThreeDImages:\n",
        "  with Image.open(image) as img:\n",
        "    img = np.array(img)\n",
        "    img=img[:,:,:-1]\n",
        "    img=(img / 127.5) - 1\n",
        "    tr_data3d[counter] = img\n",
        "    del img\n",
        "    counter += 1\n",
        "\n",
        "tr_data2d = np.zeros((len(TwoDImages), IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS), dtype=np.float32)\n",
        "counter = 0\n",
        "for image in TwoDImages:\n",
        "  with Image.open(image) as img:\n",
        "    img = np.array(img)\n",
        "    img=img[:,:,:-1]\n",
        "    img=(img / 127.5) - 1\n",
        "    tr_data2d[counter] = img\n",
        "    del img\n",
        "    counter += 1\n",
        "\n",
        "data_tf_2D = tf.convert_to_tensor(tr_data2d, np.float32)\n",
        "data_tf_3D = tf.convert_to_tensor(tr_data3d, np.float32)\n",
        "del tr_data2d\n",
        "del tr_data3d\n",
        "\n",
        "train_dataset2d_0 = tf.data.Dataset.from_tensor_slices(data_tf_2D)\n",
        "train_dataset2d = train_dataset2d_0.batch(BATCH_SIZE)\n",
        "\n",
        "train_dataset3d_0 = tf.data.Dataset.from_tensor_slices(data_tf_3D)\n",
        "train_dataset3d = train_dataset3d_0.batch(BATCH_SIZE)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time to completion (s): \", end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to completion (s):  47.00441241264343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NkQ5LDpZZ8R"
      },
      "source": [
        "# Define Deep Learning Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EtQ-S05GK_Q"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "IMG_SIZE0 = 256    # Initial model implementation only works with 256x256 - revisit up/down layers for other sizes\r\n",
        "IMG_SIZE1 = 256\r\n",
        "OUTPUT_CHANNELS = 3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUt251j_Z8ho"
      },
      "source": [
        "# Define discriminator and generator models\n",
        "# Define loss and optimizers\n",
        "# Instantiate models\n",
        "\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "\n",
        "\n",
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[IMG_SIZE0, IMG_SIZE1, OUTPUT_CHANNELS], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
        "\n",
        "\n",
        "# define loss and optimizers\n",
        "\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss\n",
        "\n",
        "\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "\n",
        "# Instantiate the models  (toggle on for GPU)\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjaEgP3bK6a5"
      },
      "source": [
        "#################\n",
        "#### ENABLE TPU VERSION OF MODEL\n",
        "\n",
        "#### ONLY USE WITH TPU\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  generator = Generator()\n",
        "  discriminator = Discriminator()\n",
        "\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "# generator = tf.contrib.tpu.keras_to_tpu_model(\n",
        "#     generator,\n",
        "#     strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "# discriminator = tf.contrib.tpu.keras_to_tpu_model(\n",
        "#     discriminator,\n",
        "#     strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNRPfnE5aS-P"
      },
      "source": [
        "## Display graphical model description\n",
        "\n",
        "# Generator().summary()\n",
        "# Discriminator().summary()\n",
        "\n",
        "# tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y-dDepoak5S"
      },
      "source": [
        "# Define Training Steps and Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-6MC4HCan4C"
      },
      "source": [
        "# set up checkpoints and define training / fit steps\n",
        "\n",
        "log_dir=\"/content/drive/My Drive/RadNET/logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "checkpoint_dir = '/content/drive/My Drive/RadNET/training_checkpoints/Production_Run1'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "\n",
        "#### summary writer not working locally, test on Colab?\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "def fit(train_3d, train_2d, epochs, chkpt_save):\n",
        "    for epoch in range(epochs):\n",
        "        display.clear_output(wait=True)\n",
        "        try:\n",
        "            print ('Time taken for epoch {} was {} sec\\n'.format(epoch - 1,\n",
        "                                                        time.time()-start))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        start = time.time()\n",
        "        print(\"Epoch \", epoch, \" running...\")\n",
        "\n",
        "        n=0\n",
        "        # Train (separate Datasets for input and target)\n",
        "        for input_image, target in tf.data.Dataset.zip((train_3d, train_2d)):\n",
        "            train_step(input_image, target, epoch)\n",
        "            if (n+1) % 100 == 0:\n",
        "                print('.', end='')\n",
        "            n +=1 \n",
        "    \n",
        "        print()\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    # saving (checkpoint) the model every chkpt_save epochs\n",
        "        if (epoch + 1) % chkpt_save == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9NjoAjpbSp7"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GRizQdMbUN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebb9a2e-41a7-4ec3-9a01-f621a79a3be4"
      },
      "source": [
        "# Load latest checkpoint if picking up / continuing training \n",
        "# Skip if starting from scratch\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6d1e1ae780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSlUzWVjbXXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d903abd-3360-4980-88b6-b44d430609d9"
      },
      "source": [
        "# Define how many epochs to run model\n",
        "EPOCHS = 99\n",
        "chkpt_save = 50\n",
        "\n",
        "# Run the model\n",
        "fit(train_dataset3d, train_dataset2d, EPOCHS, chkpt_save)\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for epoch 97 was 54.23788785934448 sec\n",
            "\n",
            "Epoch  98  running...\n",
            ".\n",
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CdiaENRc35V"
      },
      "source": [
        "# Run Predictions on Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykTl0sfEm775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31dc6c91-44fb-458f-935e-94f0b2890c8d"
      },
      "source": [
        "### Use gdown to copy zip files to local VM\n",
        "## Unpack zip files on local VM\n",
        "\n",
        "!gdown --id 1uTPGo7RPBWm5zKcV7qOAwZ1GlyT0GVRA   \n",
        "!gdown --id 12kc66DHKuBqTMr0TricOAHDb4iCJsEH0\n",
        "\n",
        "\n",
        "_2D_Test_folder_local = '/content/TEST_2D_Production_Min10_Max1_Testing-20201030T221812Z-001.zip'\n",
        "_2D_Test_target_dir = '/content/2D_testing'\n",
        "\n",
        "_3D_Test_folder_local = '/content/TEST_3D_Production_Min10_Max1_Testing-20201030T221810Z-001.zip'\n",
        "_3D_Test_target_dir = '/content/3D_testing'\n",
        "\n",
        "shutil.unpack_archive(_2D_Test_folder_local, _2D_Test_target_dir)\n",
        "shutil.unpack_archive(_3D_Test_folder_local, _3D_Test_target_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uTPGo7RPBWm5zKcV7qOAwZ1GlyT0GVRA\n",
            "To: /content/TEST_3D_Production_Min10_Max1_Testing-20201030T221810Z-001.zip\n",
            "8.16MB [00:00, 30.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12kc66DHKuBqTMr0TricOAHDb4iCJsEH0\n",
            "To: /content/TEST_2D_Production_Min10_Max1_Testing-20201030T221812Z-001.zip\n",
            "8.75MB [00:00, 24.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKKC-53KdpAq"
      },
      "source": [
        "# 3D test images to enhance\n",
        "\n",
        "## if using Drive:\n",
        "# _3D_test_path = \"/content/drive/My Drive/RadNET/GAN data/Dev_Testing_Data/3D_testing\"\n",
        "\n",
        "## if using local VM:\n",
        "_3D_test_path = '/content/3D_testing/3D_Production_Min10_Max1_Testing'\n",
        "\n",
        "output_prediction_path = \"/content/drive/My Drive/RadNET/GAN data/Production_Run1/Predictions_v110920/\"\n",
        "\n",
        "\n",
        "\n",
        "TestImages = []\n",
        "for root, dirs, files in os.walk(_3D_test_path):\n",
        "    for name in files:\n",
        "        if name.endswith(\".png\"):\n",
        "            filename = os.path.join(root, name)\n",
        "            name = name.split('.')[0]\n",
        "            TestImages.append([name, filename])\n",
        "\n",
        "## Make sure these parameters match image generation step\n",
        "colormap = 'jet'\n",
        "min_val = -10\n",
        "max_val = 1\n",
        "shading = 'flat'\n",
        "\n",
        "for image in TestImages:\n",
        "    with Image.open(image[1]) as img:\n",
        "      img = np.array(img)\n",
        "      img=img[:,:,:-1]\n",
        "      img=(img / 127.5) - 1\n",
        "      testing = np.expand_dims(img, axis=0)\n",
        "      model_test = generator.predict(testing)\n",
        "      model_test = np.squeeze(model_test)\n",
        "      model_test = (model_test + 1) * 127.5\n",
        "      model_test = model_test.astype(np.uint8)\n",
        "      plt.imsave(os.path.join(output_prediction_path,str(image[0]) + \"_prediction.png\"), \n",
        "                model_test, cmap=colormap, vmin=min_val, vmax=max_val)\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv4VUuv1HFxn"
      },
      "source": [
        "## Run predictions on test 3D profile section\r\n",
        "\r\n",
        "\r\n",
        "_3D_test_path = '/content/drive/MyDrive/RadNET/GAN data/Enhanced_Line_Section/Line1308201000_3Dtraces/'\r\n",
        "\r\n",
        "output_prediction_path = \"/content/drive/MyDrive/RadNET/GAN data/Enhanced_Line_Section/Enhanced_Traces/\"\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "TestImages = []\r\n",
        "for root, dirs, files in os.walk(_3D_test_path):\r\n",
        "    for name in files:\r\n",
        "        if name.endswith(\".png\"):\r\n",
        "            filename = os.path.join(root, name)\r\n",
        "            name = name.split('.')[0]\r\n",
        "            TestImages.append([name, filename])\r\n",
        "\r\n",
        "## Make sure these parameters match image generation step\r\n",
        "colormap = 'jet'\r\n",
        "min_val = -10\r\n",
        "max_val = 1\r\n",
        "shading = 'flat'\r\n",
        "\r\n",
        "for image in TestImages:\r\n",
        "    with Image.open(image[1]) as img:\r\n",
        "      img = np.array(img)\r\n",
        "      img=img[:,:,:-1]\r\n",
        "      img=(img / 127.5) - 1\r\n",
        "      testing = np.expand_dims(img, axis=0)\r\n",
        "      model_test = generator.predict(testing)\r\n",
        "      model_test = np.squeeze(model_test)\r\n",
        "      model_test = (model_test + 1) * 127.5\r\n",
        "      model_test = model_test.astype(np.uint8)\r\n",
        "      plt.imsave(os.path.join(output_prediction_path,str(image[0]) + \"_prediction.png\"), \r\n",
        "                model_test, cmap=colormap, vmin=min_val, vmax=max_val)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2prhcSrbV-v5"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skEsPSkFWBw5"
      },
      "source": [
        "# %load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}